name: llm_server

services:
  qwen3-0.6b:
    build:
      context: ./models/qwen3-0.6b/vllm
      dockerfile: Containerfile
    restart: always
    image: vllm/vllm-qwen3-0.6b:v0.10.1.1
    container_name: vllm-qwen3-0.6b
    networks:
      - llm_server
    ports:
      - "8000:8000"
    volumes:
      - <your_model_cache>:/root/.cache/huggingface/hub
    tty: true
    stdin_open: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

networks:
  llm_server:
    driver: bridge
